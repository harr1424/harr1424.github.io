<!doctype html>
<html lang="en">
<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1" name="viewport">

    <!-- Bootstrap CSS -->
    <link crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css"
          integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" rel="stylesheet">

    <title>Honorable Mentions</title>
</head>
<body>
<div class="container">
    <div class="row ">
        <h1><a href="index.html">jharrington.dev</a></h1>
    </div>
    <div class="pt-5"></div>
    <div class="row">
        <div class="col-md-6">
            <h3>
                <a href="https://github.com/harr1424/web_crawler">Web Crawler</a>
            </h3>
            <p>
                This python script was written to crawl a website in order to find any anchor tags that
                link to <code>.zip</code> file downloads. The address of these files is stored in a list and once
                crawling is completed the script will download each file using the <code>requests</code> library.
            </p>
            <pre>
                <code>
import requests
import re
import os
from bs4 import BeautifulSoup
from tqdm import tqdm

target_url = # website to crawl

parent_dir = # location to store files locally

headers = {
'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:94.0) Gecko/20100101 Firefox/94.0',
}

def crawl():
    target_links = []
    print("Crawling Main Page")
    response = requests.get(target_url, headers=headers)
    soup = BeautifulSoup(response.content, 'html5lib')
    indirect_links = soup.find_all('a', {'href': re.compile(r'/en.*-.*/')})
    authors = [link['href'] for link in indirect_links]
    print("Crawling Secondary Pages")
    for each in tqdm(authors):
        response = requests.get(each, headers=headers)
        soup = BeautifulSoup(response.content, 'html5lib')
        download_links = soup.find_all('a', {'href': re.compile(r'.*.zip')})
        for link in download_links:
            target_links.append(link['href'])
    target_links = list(set(target_links))
    print(len(target_links), "Files available for download...")
    return target_links


    def download(target_links):
        print("Beginning Downloads...")
        for link in target_links:
            author = link.split('/')[-2]
            file_name = link.split('/')[-1]
            dir_path = os.path.join(parent_dir, author)
            if not os.path.exists(dir_path):
                os.mkdir(dir_path)
            file_path = os.path.join(dir_path, file_name)
            if not os.path.isfile(file_path):
                print("Downloading file:%s from %s" % (file_name, author))
                response = requests.get(link, stream=True)
                with open(file_path, "wb+") as f:
                    for chunk in tqdm(response.iter_content(chunk_size=1024 * 1024)):
                        if chunk:
                            f.write(chunk)
                print("%s downloaded!\n" % file_name)
        print("All files downloaded!")
        return


if __name__ == "__main__":
    links = crawl()
    download(links)
                </code>
        </pre>
        </div>
    </div>
</div>


<!-- Option 1: Bootstrap Bundle with Popper -->
<script crossorigin="anonymous"
        integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p"
        src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>