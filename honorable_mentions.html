<!doctype html>
<html lang="en">

<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1" name="viewport">

    <!-- Bootstrap CSS -->
    <link crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css"
          integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" rel="stylesheet">

    <title>Honorable Mentions</title>
</head>

<body>
<div class="container">
    <div class="row ">
        <div class="dropdown">
            <a class="btn btn-secondary btn-lg dropdown-toggle" href="#" role="button" id="dropdownMenuLink"
               data-bs-toggle="dropdown" aria-expanded="false">
                Portfolio Contents
            </a>

            <ul class="dropdown-menu" aria-labelledby="dropdownMenuLink">
                <li><a class="dropdown-item" href="index.html">Home</a></li>
                <li>
                    <hr class="dropdown-divider">
                </li>
                <li><a class="dropdown-item" href="mobile_app_dev.html">Mobile Application Development</a></li>
                <li><a class="dropdown-item" href="cloud_computing.html">Cloud Computing</a></li>
                <li><a class="dropdown-item" href="3.html">Topic 3</a></li>
                <li><a class="dropdown-item" href="4.html">Topic 4</a></li>
                <li><a class="dropdown-item" href="5.html">Topic 5</a></li>
                <li>
                    <hr class="dropdown-divider">
                </li>
                <li><a class="dropdown-item" href="honorable_mentions.html">Honorable Mentions</a></li>
                <li><a class="dropdown-item" href="reflective_essay.html">Reflective Essay</a></li>
                <li><a class="dropdown-item" href="CV.html">Cirriculum Vitae</a></li>
            </ul>
        </div>
    </div>
    <div class="pt-5"></div>
    <div class="row">
        <div class="col-md-6">
            <h3>
                <a href="https://github.com/harr1424/web_crawler">Web Crawler</a>
            </h3>
            <p>
                This python script was written to crawl a website in order to find any anchor tags that
                link to <code>.zip</code> file downloads. The addresses of these files is stored in a list and once
                crawling is completed the script will download each file using the <code>requests</code> library.
            </p>
            <pre>
                <code>
import requests
import re
import os
from bs4 import BeautifulSoup
from tqdm import tqdm

target_url = # website to crawl

parent_dir = # location to store files locally

headers = {
'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:94.0) Gecko/20100101 Firefox/94.0',
}

def crawl():
    target_links = []
    print("Crawling Main Page")
    response = requests.get(target_url, headers=headers)
    soup = BeautifulSoup(response.content, 'html5lib')
    indirect_links = soup.find_all('a', {'href': re.compile(r'/en.*-.*/')})
    authors = [link['href'] for link in indirect_links]
    print("Crawling Secondary Pages")
    for each in tqdm(authors):
        response = requests.get(each, headers=headers)
        soup = BeautifulSoup(response.content, 'html5lib')
        download_links = soup.find_all('a', {'href': re.compile(r'.*.zip')})
        for link in download_links:
            target_links.append(link['href'])
    target_links = list(set(target_links))
    print(len(target_links), "Files available for download...")
    return target_links


    def download(target_links):
        print("Beginning Downloads...")
        for link in target_links:
            author = link.split('/')[-2]
            file_name = link.split('/')[-1]
            dir_path = os.path.join(parent_dir, author)
            if not os.path.exists(dir_path):
                os.mkdir(dir_path)
            file_path = os.path.join(dir_path, file_name)
            if not os.path.isfile(file_path):
                print("Downloading file:%s from %s" % (file_name, author))
                response = requests.get(link, stream=True)
                with open(file_path, "wb+") as f:
                    for chunk in tqdm(response.iter_content(chunk_size=1024 * 1024)):
                        if chunk:
                            f.write(chunk)
                print("%s downloaded!\n" % file_name)
        print("All files downloaded!")
        return


if __name__ == "__main__":
    links = crawl()
    download(links)
                </code>
        </pre>
        </div>
    <div class="col-md-6">
        <h3>
            <a href="https://twitter.com/harr1424">Twitter Air Quality Bot</a>
        </h3>
        <p>
            This python script contacts the US Environmental Protection Agency's
            AirNow API to retrieve current Missoula Air Quality. This data is used to
            format a <i>Tweet</i> informing followers of current air quality.
        </p>
        <pre>
                <code>
import tweepy
import requests
import time
import sys

auth = tweepy.OAuth1UserHandler(API_KEY, API_KEY_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET)
api = tweepy.API(auth, wait_on_rate_limit=True)

# Provide reference to airnow.gov at the end of each tweet 
airnow = 'https://www.airnow.gov/?city=Missoula&state=MT&country=USA'

def get_AQI(): 
    # contact API endpoint 
    response = requests.get('https://www.airnowapi.org/aq/observation/zipCode/current/?format=application/json&zipCode=59801&distance=25&API_KEY=664ACC75-2DBA-4812-A360-619F57BBA12A')
    data = response.json()

    # response payload for each pollutant 
    o3 = data[0]
    pm25 = data[1]
    pm10 = data[2]

    # ensure response payload matches expected format 
    if o3['ParameterName'] != 'O3' or pm25['ParameterName'] != 'PM2.5' or pm10['ParameterName'] != 'PM10':
        print("API returned unexpected payload!")
        sys.exit(-1)

    # AQI for each pollutant
    o3_aqi = data[0]['AQI']
    pm25_aqi = data[1]['AQI']
    pm10_aqi = data[2]['AQI']

    # format time correctly
    am_pm = 'PM' if o3['HourObserved'] > 11 else 'AM'
    latest_measure = o3['HourObserved'] % 12

    # dict containing pollutant, aqi as key, value
    all_polutants = {
        'Ozone': o3_aqi,
        'PM 2.5' : pm25_aqi, 
        'PM 10' : pm10_aqi
    }

    # get the top pollutant
    top_pollutant = max(all_polutants, key=all_polutants.get)
    # get the value corresponding to the top pollutant
    AQI = all_polutants[top_pollutant]

    # format the tweet 
    tweet = f'Current Missoula Air Quality Index is {AQI}. The major pollutant is {top_pollutant}. This measurement occurred at {latest_measure} {am_pm}. \n{airnow}'

    return tweet


if __name__ == "__main__": 
    while True: 
        try: 
            api.verify_credentials()
            print("Authentication succeeded")
            try: 
                api.update_status(status=get_AQI())
            except Exception as e:
                print(e)
        except Exception as e:
            print("Authentication failed")
            print(e)
        # run this script every 15 minutes
        # duplicate tweets will be rejected by the Twitter API
        time.sleep(15 * 60)
        </code>
        </pre>
    </div>
</div>


<!-- Option 1: Bootstrap Bundle with Popper -->
<script crossorigin="anonymous" integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p"
        src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
</body>

</html>