<!doctype html>
<html lang="en">

<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1" name="viewport">

    <link rel="stylesheet" href="style.css">

    <!-- Bootstrap CSS -->
    <link crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css"
        integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" rel="stylesheet">


    <title>Web Crawler</title>
</head>

<body>
    <div class="container">
        <div class="row ">
            <div class="dropdown">
                <a class="btn btn-secondary btn-lg dropdown-toggle" href="#" role="button" id="dropdownMenuLink"
                    data-bs-toggle="dropdown" aria-expanded="false">
                    Portfolio Contents
                </a>

                <ul class="dropdown-menu" aria-labelledby="dropdownMenuLink">
                    <li><a class="dropdown-item" href="index.html">Home</a></li>
                    <li>
                        <hr class="dropdown-divider">
                    </li>
                    <li><a class="dropdown-item" href="mobile_app_dev.html">Mobile Application Development</a></li>
                    <li><a class="dropdown-item" href="cloud_computing.html">Cloud Computing</a></li>
                    <li><a class="dropdown-item" href="3.html">Topic 3</a></li>
                    <li><a class="dropdown-item" href="4.html">Topic 4</a></li>
                    <li><a class="dropdown-item" href="5.html">Topic 5</a></li>
                    <li><a class="dropdown-item" href="reflective_essay.html">Reflective Essay</a></li>
                    <li><a class="dropdown-item" href="CV.html">Cirriculum Vitae</a></li>
                    <li>
                        <hr class="dropdown-divider">
                    <li><a class="dropdown-item" href="orange_weather.html">iOS Networking - Orange Weather</a></li>
                    <li><a class="dropdown-item" href="list_maker.html">iOS Data Persistence - List Maker</a></li>
                    <li><a class="dropdown-item" href="honorable_mentions.html">Honorable Mentions</a></li>


                    </li>

                </ul>
            </div>
        </div>
        <div class="pt-5"></div>
        <div class="row">
            <div class="col-md-6">
                <h3>
                    <a href="https://github.com/harr1424/web_crawler">Web Crawler</a>
                </h3>
                <ul>
                    <li>
                        This python script was written to crawl a website in order to find any anchor tags that
                        link to <code>.zip</code> file downloads. The addresses of these files is stored in a list and once
                        crawling is completed the script will download each file using the <code>requests</code> module.
                    </li>
                    <li>
                        The program user must specify a url to crael as well as where on their local machine that they 
                        wish to store downloaded files.
                    </li>
                    <li>
                        Many sites block requests from clients that do not provide a user agent string in the reguest header. For this 
                        reason I have specified a user agent agent representing a commonly used web browser. 
                    </li>
                    <li>
                        The <code>crawl()</code> function reguests the html content of a web page and uses the <code>BeautifulSoup</code> 
                        module to parse out each anchor tag's <code>href</code> attribute. 
                    </li>
                    <li>
                        <code>BeautifulSoup</code> has a <code>find</code> method which accepts a regular expression as an arguemnt. 
                        Here this is used to find all anchor tag <code>href</code> attributes containing any pattern typical of 
                        a english page on this particular site: an <code>/en</code> subdirectory followed by any number of any characters. 
                    </li>
                    <li>
                        These links are stored in a list named <code>authors</code>.
                    </li>
                    <li>
                        The <code>authors</code> list is then subsequently crawled for any anchor tages with <code>href</code> attributes 
                        containing a <code>.zip</code> file extension. 
                    </li>
                    <li>
                        These links are then stored in a list named <code>target_links</code>.
                    </li>
                    <li>
                        <a href="https://github.com/tqdm/tqdm">TQDM</a> is a module that provides a graphical represention of the 
                        program's process. It can be used by simply wrapping any sort of iterable in it's invocation as follows: 
                        <code>
                            <pre>
for each in tqdm(some_list):
    # do something
                            </pre>
                        </code>
                    </li>
                    <li>
                        The <code>download()</code> function accepts as a single parameter a list of urls to download. 
                        In this case, it is the list of all <code>.zip</code> files found on all english subdirectories of 
                        the target website. 
                    </li>
                    <li>
                        The url for each file download is used to create a filename used to save the download onto the 
                        program usr's local machine. 
                    </li>
                    <li>
                        The program user must have specified a path at which to save all downloads at the top of the script. 
                        If this path does not exist, it will be created. 
                    </li>
                    <li>
                        If the file does not already exist at the specified path, it will be downloaded. Because most 
                        files available for download on the internet are of considerable size, the files are broken into 
                        1024MB chunks as they are downloaded and written to disk incrementally instead of attempting to write the entire file to disk at once. 
                    </li>
                    <li>
                        As this process can take a significant amount of time, <code>TQDM</code> has again been used in order to provide the 
                        program user with a progress view. 
                    </li>
                </ul>
            </div>
            <div class="col-md-6">
                <pre>
                <code>
import requests
import re
import os
from bs4 import BeautifulSoup
from tqdm import tqdm

# The site you wish to crawl for downloads
target_url = "target_url"

# The location on your local machine to save downloads
parent_dir = "/Users/user/" 

# Some sites block requests if a user agent string is not providded
headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:94.0) Gecko/20100101 Firefox/94.0',
}


"""
Crawls a web page and stores anchor tag href attributes in a list for subsequent download
"""
def crawl():
    target_links = []
    print("Crawling Main Page")
    response = requests.get(target_url, headers=headers)
    soup = BeautifulSoup(response.content, 'html5lib')
    indirect_links = soup.find_all('a', {'href': re.compile(r'/en.*-.*/')})
    authors = [link['href'] for link in indirect_links]
    print("Crawling Secondary Pages")
    for each in tqdm(authors):
        response = requests.get(each, headers=headers)
        soup = BeautifulSoup(response.content, 'html5lib')
        download_links = soup.find_all('a', {'href': re.compile(r'.*.zip')})
        for link in download_links:
            target_links.append(link['href'])
    target_links = list(set(target_links))
    print(len(target_links), "Files available for download...")
    return target_links


def download(target_links):
    print("Beginning Downloads...")
    for link in target_links:
        author = link.split('/')[-2]
        file_name = link.split('/')[-1]
        dir_path = os.path.join(parent_dir, author)
        if not os.path.exists(dir_path):
            os.mkdir(dir_path)
        file_path = os.path.join(dir_path, file_name)
        if not os.path.isfile(file_path):
            print("Downloading file:%s from %s" % (file_name, author))
            response = requests.get(link, stream=True)
            with open(file_path, "wb+") as f:
                for chunk in tqdm(response.iter_content(chunk_size=1024 * 1024)):
                    if chunk:
                        f.write(chunk)
            print("%s downloaded!\n" % file_name)
    print("All files downloaded!")
    return


if __name__ == "__main__":
    links = crawl()
    download(links)
                </code>
        </pre>
            </div>
        </div>
    </div>
        <!-- Option 1: Bootstrap Bundle with Popper -->
        <script crossorigin="anonymous" integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p"
        src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
</body>

</html>